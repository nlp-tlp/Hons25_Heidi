import streamlit as st
import json

import sys
import os
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from pipeline.controller import query

st.set_page_config(page_title="Semi-Structured RAG demo", layout="wide")

# Chat and settings history
if "chat_history_planner" not in st.session_state:
    st.session_state.chat_history_planner = []
if "schema_context" not in st.session_state:
    st.session_state.schema_context = "Default schema context..."  # fallback if never set

st.title("Query Interface")
st.markdown("This chat runs a **Planning/Routing** strategy. Given the user's question and some appended context, the configured LLM is made to decompose the question into a plan for directing Neo4j data between structured Cypher queries and a prewritten Cypher query employging Neo4j's vector similarity functionality. The queries and search terms are also generated by the LLM. After relevant information is retrieved, they are again passed to an LLM for generating a final response.")

# Sidebar model selection
with st.sidebar:
    st.markdown("### Model Configuration")

    # degaults
    if "active_retriever_model" not in st.session_state:
            st.session_state.active_retriever_model = "llama3.2"
    if "active_generator_model" not in st.session_state:
        st.session_state.active_generator_model = "llama3.2"
    if "active_embedding_model" not in st.session_state:
            st.session_state.active_embedding_model = "text-embedding-3-small"

    # Temporary selection before save
    retriever = st.selectbox(
        "Retriever LLM",
        options=["gpt-4o", "llama3.2"],
        index=["gpt-4o", "llama3.2"].index(st.session_state.active_retriever_model),
        key="temp_retriever_model"
    )

    generator = st.selectbox(
        "Generator LLM",
        options=["gpt-4o", "llama3.2"],
        index=["gpt-4o", "llama3.2"].index(st.session_state.active_generator_model),
        key="temp_generator_model"
    )

    embedder = st.selectbox(
        "Embedding model",
        options=["text-embedding-3-small", "mxbai-embed-large"],
        index=["text-embedding-3-small", "mxbai-embed-large"].index(st.session_state.active_embedding_model),
        key="temp_embedding_model"
    )

    if st.button("Apply settings for next submit"):
        st.session_state.active_retriever_model = st.session_state.temp_retriever_model
        st.session_state.active_generator_model = st.session_state.temp_generator_model
        st.session_state.active_embedding_model = st.session_state.temp_embedding_model
        st.success("Settings applied successfully. These will be used on your next query.")

# Display chat history
for entry in st.session_state.chat_history_planner:
    with st.chat_message(entry["role"]):
        if "config" in entry:
            st.markdown(f"**Configuration:** Retriever: `{entry['config']['retriever_model']}` | Generator: `{entry['config']['generator_model']}` | Embedding model: `{entry['config']['embedding_model']}`")

        st.markdown(entry["msg"])

        if "plan" in entry and "error" in entry:
            with st.expander("Show Plan and Queries"):
                st.code(entry["plan"], wrap_lines=True, height=200)
            with st.expander("Show Error"):
                st.code(entry["error"], wrap_lines=True, height=200)
        elif "plan" in entry and "raw" in entry:
            with st.expander("Show Plan and Queries"):
                st.code(json.dumps(entry["plan"], indent=4), language="json", height=200)
            with st.expander("Show Raw Retrieved Information"):
                st.code(json.dumps(entry["raw"], indent=4), language="json", height=200)

# User input
question = st.chat_input("Ask a question...")
if question:
    with st.chat_message("user"):
        st.markdown(question)

    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            config_snapshot = {
                "retriever_model": st.session_state.active_retriever_model,
                "generator_model": st.session_state.active_generator_model,
                "embedding_model": st.session_state.active_embedding_model
            }

            plan, last_results, response, error = query(question, st.session_state.schema_context, strategy="planning_routing")

    st.session_state.chat_history_planner.append({"role": "user", "msg": question})
    if error:
        st.session_state.chat_history_planner.append({"role": "assistant", "msg": response, "plan": plan, "error": error, "config": config_snapshot})
    else:
        st.session_state.chat_history_planner.append({"role": "assistant", "msg": response, "plan": plan, "raw": last_results, "config": config_snapshot})
    st.rerun()