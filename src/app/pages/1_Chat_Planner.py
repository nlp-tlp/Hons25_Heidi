import streamlit as st
import json

from app.controller import rag_query, get_chat_model_names, get_embedder_names

st.set_page_config(page_title="Semi-Structured RAG demo", layout="wide")

st.title("Query Interface")
st.markdown("This chat runs a **Planning/Routing** strategy. Given the user's question and some appended context, the configured LLM is made to decompose the question into a plan for directing Neo4j data between structured Cypher queries and a prewritten Cypher query employing Neo4j's vector similarity functionality. The queries and search terms are also generated by the LLM. After relevant information is retrieved, they are again passed to an LLM for generating a final response.")

# Chat and settings history
if "chat_history_planner" not in st.session_state:
    st.session_state.chat_history_planner = []

# Sidebar model selection
with st.sidebar:
    st.markdown("### Model Configuration")

    # defaults
    if "active_retriever_model" not in st.session_state:
        st.session_state.active_retriever_model = "gpt-4.1"
    if "active_generator_model" not in st.session_state:
        st.session_state.active_generator_model = "gpt-4o"
    if "active_embedding_model" not in st.session_state:
        st.session_state.active_embedding_model = "text-embedding-3-small"

    with st.form("config_form", border=False, enter_to_submit=False):
        retriever_config = st.selectbox(
            "Retriever LLM",
            options=get_chat_model_names(),
            index=get_chat_model_names().index(st.session_state.active_retriever_model)
        )

        generator_config = st.selectbox(
            "Generator LLM",
            options=get_chat_model_names(),
            index=get_chat_model_names().index(st.session_state.active_generator_model)
        )

        embedding_config = st.selectbox(
            "Embedding model",
            options=get_embedder_names(),
            index=get_embedder_names().index(st.session_state.active_embedding_model),
        )

        submitted = st.form_submit_button("Apply settings for next submit")
        if submitted:
            st.session_state.active_retriever_model = retriever_config
            st.session_state.active_generator_model = generator_config
            st.session_state.active_embedding_model = embedding_config

            st.success("Settings applied successfully. These will be used on your next query.")

# Display chat history
for entry in st.session_state.chat_history_planner:
    with st.chat_message(entry["role"]):
        if "config" in entry:
            st.markdown(f"**Configuration:** Retriever: `{entry['config']['retriever_model']}` | Generator: `{entry['config']['generator_model']}` | Embedding model: `{entry['config']['embedding_model']}`")

        st.markdown(entry["msg"])

        if "plan" in entry and "error" in entry:
            with st.expander("Show Plan and Queries"):
                st.code(entry["plan"], wrap_lines=True, height=200)
            with st.expander("Show Error"):
                st.code(entry["error"], wrap_lines=True, height=200)
        elif "plan" in entry and "raw" in entry:
            with st.expander("Show Plan and Queries"):
                st.code(json.dumps(entry["plan"], indent=4), language="json", height=200)
            with st.expander("Show Raw Retrieved Information"):
                st.code(json.dumps(entry["raw"], indent=4), language="json", height=200)

# User input
question = st.chat_input("Ask a question...")
if question:
    with st.chat_message("user"):
        st.markdown(question)

    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            config_snapshot = {
                "retriever_model": st.session_state.active_retriever_model,
                "generator_model": st.session_state.active_generator_model,
                "embedding_model": st.session_state.active_embedding_model
            }

            plan, last_results, response, error = rag_query(
                question,
                strategy="planning_routing",
                retriever_model=st.session_state.active_retriever_model,
                generator_model=st.session_state.active_generator_model,
                embedder=st.session_state.active_embedding_model
            )

    st.session_state.chat_history_planner.append({"role": "user", "msg": question})
    if error:
        st.session_state.chat_history_planner.append({"role": "assistant", "msg": response, "plan": plan, "error": error, "config": config_snapshot})
    else:
        st.session_state.chat_history_planner.append({"role": "assistant", "msg": response, "plan": plan, "raw": last_results, "config": config_snapshot})
    st.rerun()